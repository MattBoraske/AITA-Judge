-----
TO DO
-----

1. toxicity eval for justifications

2. refactor vs_creation script

3. add code for dataset collection/creation (helps validate legitimacy of HF dataset)
    - collection = making initial data files using reddit API
    - creation = refinements done to create HF dataset used to train model
    - store initial data file on S3 and include links to download them in the README

4. consider what an a complete end to end experiment pipeline would look like
    - not sure if its worth it though... would be if we had a constant source of new data to index
        - maybe it would be though to run experiments on vector stores containing different 1) data subsets or 2) embeddings

5. add code for dataset collection/creation (helps validate legitimacy of HF dataset)
    - collection = making initial data files using reddit API
    - creation = refinements done to create HF dataset used to train model
    - store initial data file on S3 and include links to download them in the README
