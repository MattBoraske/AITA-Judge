-----
TO DO
-----

1. toxicity eval for justifications
    - store toxicity classifications prior to plotting
    - finish and add in toxicity plotting

2. add code for dataset collection/creation (helps validate legitimacy of HF dataset)
    - collection = making initial data files using reddit API
    - creation = refinements done to create HF dataset used to train model
    - store initial data file on S3 and include links to download them in the README

3. consider what an a complete end to end experiment pipeline would look like
    - not sure if its worth it though... would be if we had a constant source of new data to index
        - maybe it would be though to run experiments on vector stores containing different 1) data subsets or 2) embeddings

4. add code for dataset collection/creation (helps validate legitimacy of HF dataset)
    - collection = making initial data files using reddit API
    - creation = refinements done to create HF dataset used to train model
    - store initial data file on S3 and include links to download them in the README

5. fix requirements.txt