2025-01-04 22:01:05,157 - __main__ - INFO - Starting AITA Agent evaluation script
2025-01-04 22:01:05,159 - __main__ - INFO - Setting up telemetry
2025-01-04 22:01:05,237 - __main__ - INFO - Telemetry setup completed successfully
2025-01-04 22:01:05,238 - __main__ - INFO - Starting evaluation process
2025-01-04 22:01:05,238 - __main__ - INFO - Initializing Evaluation Utility
2025-01-04 22:01:05,238 - src.agent.evaluation.eval_util - INFO - Initialized Evaluation_Utility
2025-01-04 22:01:05,238 - __main__ - INFO - Initializing AITA Basic Agent workflow
2025-01-04 22:01:05,238 - __main__ - INFO - Loading dataset from MattBoraske/reddit-AITA-submissions-and-comments-multiclass
2025-01-04 22:06:57,709 - __main__ - INFO - Starting AITA Agent evaluation script
2025-01-04 22:06:57,712 - __main__ - INFO - Setting up telemetry
2025-01-04 22:06:57,783 - __main__ - INFO - Telemetry setup completed successfully
2025-01-04 22:06:57,789 - __main__ - INFO - Starting evaluation process
2025-01-04 22:06:57,790 - __main__ - INFO - Initializing Evaluation Utility
2025-01-04 22:06:57,790 - src.agent.evaluation.eval_util - INFO - Initialized Evaluation_Utility
2025-01-04 22:06:57,790 - __main__ - INFO - Initializing AITA Basic Agent workflow
2025-01-04 22:06:57,790 - __main__ - INFO - Loading dataset from MattBoraske/reddit-AITA-submissions-and-comments-multiclass
2025-01-04 22:07:05,181 - __main__ - INFO - Dataset size after filtering: 9867
2025-01-04 22:07:05,181 - __main__ - INFO - Creating test set
2025-01-04 22:07:05,182 - src.agent.evaluation.eval_util - INFO - Creating test set using balanced sampling strategy
2025-01-04 22:07:05,182 - src.agent.evaluation.eval_util - INFO - Creating balanced test set with 3 samples per class
2025-01-04 22:07:05,211 - src.agent.evaluation.eval_util - INFO - Successfully created test set with 12 samples
2025-01-04 22:07:05,211 - __main__ - INFO - Final test set size: 12
2025-01-04 22:07:05,211 - __main__ - INFO - Starting response collection
2025-01-04 22:07:05,219 - src.agent.evaluation.eval_util - INFO - Starting response collection for 12 samples
2025-01-04 22:07:06,797 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:06,923 - openinference.instrumentation.llama_index._handler - ERROR - Error serializing to JSON: PydanticSerializationError: Error calling function `custom_model_dump`: TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer'
Traceback (most recent call last):
  File "/Users/mattboraske/.pyenv/versions/3.11.9/lib/python3.11/site-packages/openinference/instrumentation/llama_index/_handler.py", line 253, in process_output
    self[OUTPUT_VALUE] = result.model_dump_json(exclude_unset=True)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mattboraske/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pydantic/main.py", line 477, in model_dump_json
    return self.__pydantic_serializer__.to_json(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.PydanticSerializationError: Error serializing to JSON: PydanticSerializationError: Error calling function `custom_model_dump`: TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer'
2025-01-04 22:07:08,364 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:12,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:15,527 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:16,969 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:18,908 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:20,507 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:22,182 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:23,514 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:25,356 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:28,129 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:29,777 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-01-04 22:07:29,992 - src.agent.evaluation.eval_util - INFO - Response collection completed. Successful: 12, Failed: 0
2025-01-04 22:07:29,996 - __main__ - INFO - Collected 12 responses
2025-01-04 22:07:29,996 - __main__ - INFO - Starting evaluation of responses
2025-01-04 22:07:29,997 - src.agent.evaluation.eval_util - INFO - Starting evaluation
2025-01-04 22:07:29,997 - src.agent.evaluation.eval_util - INFO - Performing basic evaluation
2025-01-04 22:07:29,997 - src.agent.evaluation.eval_util - INFO - Evaluating classifications...
2025-01-04 22:07:38,145 - src.agent.evaluation.eval_util - INFO - Starting classification evaluation
2025-01-04 22:07:38,147 - src.agent.evaluation.eval_util - INFO - Generating classification report
2025-01-04 22:07:38,171 - src.agent.evaluation.eval_util - INFO - Generating confusion matrix
2025-01-04 22:07:38,862 - src.agent.evaluation.eval_util - INFO - Calculating Matthews Correlation Coefficient
2025-01-04 22:07:39,285 - src.agent.evaluation.eval_util - INFO - Classification evaluation completed successfully
2025-01-04 22:07:39,285 - src.agent.evaluation.eval_util - INFO - Evaluating justifications...
2025-01-04 22:07:41,789 - src.agent.evaluation.eval_util - INFO - Starting justification evaluation
2025-01-04 22:07:41,790 - src.agent.evaluation.eval_util - INFO - Calculating ROUGE scores
2025-01-04 22:07:42,224 - absl - INFO - Using default tokenizer.
2025-01-04 22:07:42,357 - src.agent.evaluation.eval_util - INFO - Calculating BLEU scores
2025-01-04 22:07:43,205 - src.agent.evaluation.eval_util - INFO - Starting toxicity analysis
2025-01-04 22:08:11,210 - src.agent.evaluation.eval_util - INFO - Calculating toxicity statistics
2025-01-04 22:08:11,215 - src.agent.evaluation.eval_util - INFO - Generating toxicity visualization
2025-01-04 22:08:11,215 - src.agent.evaluation.eval_util - INFO - Generating toxicity score visualization
2025-01-04 22:08:12,183 - src.agent.evaluation.eval_util - INFO - Toxicity plot saved to eval_results/basic-test/toxicity_plot.png
2025-01-04 22:08:12,183 - src.agent.evaluation.eval_util - INFO - Justification evaluation completed successfully
2025-01-04 22:08:12,198 - src.agent.evaluation.eval_util - INFO - Evaluation completed successfully
2025-01-04 22:08:12,198 - __main__ - INFO - Saving responses to eval_results/basic-test/responses.json
2025-01-04 22:08:12,200 - __main__ - INFO - Evaluation process completed successfully
2025-01-04 22:08:12,288 - __main__ - INFO - Script execution completed
